{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Version w/ Transfer Learning & Fine-tuning\n",
    "For this approach, it will be used a pretrained DenseNet201, to which we will apply transfer learning and fine-tuning for our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_DIR = \"../data/\"\n",
    "IMG_DIR = DATA_DIR + \"/images/\"\n",
    "ANNOTATION_DIR = DATA_DIR + \"/annotations/\"\n",
    "SPLITS_DIR = DATA_DIR + \"/dl-split/\"\n",
    "OUT_DIR = \"./out/basic_fine_tuned/\"\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching pre-defined splits\n",
    "train_split = []\n",
    "test_split = []\n",
    "\n",
    "with open(SPLITS_DIR + \"/train.txt\") as train_split_f:\n",
    "    train_split = [line.strip(\"\\n\") for line in train_split_f.readlines()]\n",
    "\n",
    "with open(SPLITS_DIR + \"/test.txt\") as test_split_f:\n",
    "    test_split = [line.strip(\"\\n\") for line in test_split_f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label mapping\n",
    "label_encode_map = {\n",
    "    \"trafficlight\": 0,\n",
    "    \"speedlimit\": 1,\n",
    "    \"crosswalk\": 2,\n",
    "    \"stop\": 3,\n",
    "}\n",
    "\n",
    "label_decode_map = {\n",
    "    0: \"trafficlight\",\n",
    "    1: \"speedlimit\",\n",
    "    2: \"crosswalk\",\n",
    "    3: \"stop\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.road_sign_dataset import RoadSignDataset\n",
    "\n",
    "# Training dataset\n",
    "training_data = RoadSignDataset(\n",
    "    img_names=train_split,\n",
    "    img_dir=IMG_DIR,\n",
    "    annotation_dir=ANNOTATION_DIR,\n",
    "    classes=label_encode_map,\n",
    "    is_train=True,\n",
    "    multilabel=True,\n",
    "    obj_detection=False\n",
    ")\n",
    "\n",
    "# Test dataset\n",
    "testing_data = RoadSignDataset(\n",
    "    img_names=test_split,\n",
    "    img_dir=IMG_DIR,\n",
    "    annotation_dir=ANNOTATION_DIR,\n",
    "    classes=label_encode_map,\n",
    "    is_train=False,\n",
    "    multilabel=True,\n",
    "    obj_detection=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split training dataset into train and validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "\n",
    "np.random.seed(SEED)\n",
    "\n",
    "train_indices = list(range(len(training_data)))\n",
    "np.random.shuffle(train_indices)\n",
    "train_val_split = int(np.floor(0.2 * len(train_indices)))\n",
    "\n",
    "train_idx, val_idx = train_indices[train_val_split:], train_indices[:train_val_split]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "val_sampler = SubsetRandomSampler(val_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 32 # Tested on 1050TI with 4GB (can load at least 64 as well, but doesn't make sense to use 64 with low amount of data)\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=training_data,\n",
    "    sampler=train_sampler,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    shuffle=False, # Must be False because we're using a random sampler already\n",
    "    drop_last=True,\n",
    "    collate_fn=training_data.collate_fn\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    dataset=training_data,\n",
    "    sampler=val_sampler,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    shuffle=False, # Must be False because we're using a random sampler already\n",
    "    drop_last=True,\n",
    "    collate_fn=training_data.collate_fn\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=testing_data,\n",
    "    batch_size=1,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=testing_data.collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "DenseNet                                 --\n",
       "├─Sequential: 1-1                        --\n",
       "│    └─Conv2d: 2-1                       9,408\n",
       "│    └─BatchNorm2d: 2-2                  128\n",
       "│    └─ReLU: 2-3                         --\n",
       "│    └─MaxPool2d: 2-4                    --\n",
       "│    └─_DenseBlock: 2-5                  --\n",
       "│    │    └─_DenseLayer: 3-1             45,440\n",
       "│    │    └─_DenseLayer: 3-2             49,600\n",
       "│    │    └─_DenseLayer: 3-3             53,760\n",
       "│    │    └─_DenseLayer: 3-4             57,920\n",
       "│    │    └─_DenseLayer: 3-5             62,080\n",
       "│    │    └─_DenseLayer: 3-6             66,240\n",
       "│    └─_Transition: 2-6                  --\n",
       "│    │    └─BatchNorm2d: 3-7             512\n",
       "│    │    └─ReLU: 3-8                    --\n",
       "│    │    └─Conv2d: 3-9                  32,768\n",
       "│    │    └─AvgPool2d: 3-10              --\n",
       "│    └─_DenseBlock: 2-7                  --\n",
       "│    │    └─_DenseLayer: 3-11            53,760\n",
       "│    │    └─_DenseLayer: 3-12            57,920\n",
       "│    │    └─_DenseLayer: 3-13            62,080\n",
       "│    │    └─_DenseLayer: 3-14            66,240\n",
       "│    │    └─_DenseLayer: 3-15            70,400\n",
       "│    │    └─_DenseLayer: 3-16            74,560\n",
       "│    │    └─_DenseLayer: 3-17            78,720\n",
       "│    │    └─_DenseLayer: 3-18            82,880\n",
       "│    │    └─_DenseLayer: 3-19            87,040\n",
       "│    │    └─_DenseLayer: 3-20            91,200\n",
       "│    │    └─_DenseLayer: 3-21            95,360\n",
       "│    │    └─_DenseLayer: 3-22            99,520\n",
       "│    └─_Transition: 2-8                  --\n",
       "│    │    └─BatchNorm2d: 3-23            1,024\n",
       "│    │    └─ReLU: 3-24                   --\n",
       "│    │    └─Conv2d: 3-25                 131,072\n",
       "│    │    └─AvgPool2d: 3-26              --\n",
       "│    └─_DenseBlock: 2-9                  --\n",
       "│    │    └─_DenseLayer: 3-27            70,400\n",
       "│    │    └─_DenseLayer: 3-28            74,560\n",
       "│    │    └─_DenseLayer: 3-29            78,720\n",
       "│    │    └─_DenseLayer: 3-30            82,880\n",
       "│    │    └─_DenseLayer: 3-31            87,040\n",
       "│    │    └─_DenseLayer: 3-32            91,200\n",
       "│    │    └─_DenseLayer: 3-33            95,360\n",
       "│    │    └─_DenseLayer: 3-34            99,520\n",
       "│    │    └─_DenseLayer: 3-35            103,680\n",
       "│    │    └─_DenseLayer: 3-36            107,840\n",
       "│    │    └─_DenseLayer: 3-37            112,000\n",
       "│    │    └─_DenseLayer: 3-38            116,160\n",
       "│    │    └─_DenseLayer: 3-39            120,320\n",
       "│    │    └─_DenseLayer: 3-40            124,480\n",
       "│    │    └─_DenseLayer: 3-41            128,640\n",
       "│    │    └─_DenseLayer: 3-42            132,800\n",
       "│    │    └─_DenseLayer: 3-43            136,960\n",
       "│    │    └─_DenseLayer: 3-44            141,120\n",
       "│    │    └─_DenseLayer: 3-45            145,280\n",
       "│    │    └─_DenseLayer: 3-46            149,440\n",
       "│    │    └─_DenseLayer: 3-47            153,600\n",
       "│    │    └─_DenseLayer: 3-48            157,760\n",
       "│    │    └─_DenseLayer: 3-49            161,920\n",
       "│    │    └─_DenseLayer: 3-50            166,080\n",
       "│    │    └─_DenseLayer: 3-51            170,240\n",
       "│    │    └─_DenseLayer: 3-52            174,400\n",
       "│    │    └─_DenseLayer: 3-53            178,560\n",
       "│    │    └─_DenseLayer: 3-54            182,720\n",
       "│    │    └─_DenseLayer: 3-55            186,880\n",
       "│    │    └─_DenseLayer: 3-56            191,040\n",
       "│    │    └─_DenseLayer: 3-57            195,200\n",
       "│    │    └─_DenseLayer: 3-58            199,360\n",
       "│    │    └─_DenseLayer: 3-59            203,520\n",
       "│    │    └─_DenseLayer: 3-60            207,680\n",
       "│    │    └─_DenseLayer: 3-61            211,840\n",
       "│    │    └─_DenseLayer: 3-62            216,000\n",
       "│    │    └─_DenseLayer: 3-63            220,160\n",
       "│    │    └─_DenseLayer: 3-64            224,320\n",
       "│    │    └─_DenseLayer: 3-65            228,480\n",
       "│    │    └─_DenseLayer: 3-66            232,640\n",
       "│    │    └─_DenseLayer: 3-67            236,800\n",
       "│    │    └─_DenseLayer: 3-68            240,960\n",
       "│    │    └─_DenseLayer: 3-69            245,120\n",
       "│    │    └─_DenseLayer: 3-70            249,280\n",
       "│    │    └─_DenseLayer: 3-71            253,440\n",
       "│    │    └─_DenseLayer: 3-72            257,600\n",
       "│    │    └─_DenseLayer: 3-73            261,760\n",
       "│    │    └─_DenseLayer: 3-74            265,920\n",
       "│    └─_Transition: 2-10                 --\n",
       "│    │    └─BatchNorm2d: 3-75            3,584\n",
       "│    │    └─ReLU: 3-76                   --\n",
       "│    │    └─Conv2d: 3-77                 1,605,632\n",
       "│    │    └─AvgPool2d: 3-78              --\n",
       "│    └─_DenseBlock: 2-11                 --\n",
       "│    │    └─_DenseLayer: 3-79            153,600\n",
       "│    │    └─_DenseLayer: 3-80            157,760\n",
       "│    │    └─_DenseLayer: 3-81            161,920\n",
       "│    │    └─_DenseLayer: 3-82            166,080\n",
       "│    │    └─_DenseLayer: 3-83            170,240\n",
       "│    │    └─_DenseLayer: 3-84            174,400\n",
       "│    │    └─_DenseLayer: 3-85            178,560\n",
       "│    │    └─_DenseLayer: 3-86            182,720\n",
       "│    │    └─_DenseLayer: 3-87            186,880\n",
       "│    │    └─_DenseLayer: 3-88            191,040\n",
       "│    │    └─_DenseLayer: 3-89            195,200\n",
       "│    │    └─_DenseLayer: 3-90            199,360\n",
       "│    │    └─_DenseLayer: 3-91            203,520\n",
       "│    │    └─_DenseLayer: 3-92            207,680\n",
       "│    │    └─_DenseLayer: 3-93            211,840\n",
       "│    │    └─_DenseLayer: 3-94            216,000\n",
       "│    │    └─_DenseLayer: 3-95            220,160\n",
       "│    │    └─_DenseLayer: 3-96            224,320\n",
       "│    │    └─_DenseLayer: 3-97            228,480\n",
       "│    │    └─_DenseLayer: 3-98            232,640\n",
       "│    │    └─_DenseLayer: 3-99            236,800\n",
       "│    │    └─_DenseLayer: 3-100           240,960\n",
       "│    │    └─_DenseLayer: 3-101           245,120\n",
       "│    │    └─_DenseLayer: 3-102           249,280\n",
       "│    │    └─_DenseLayer: 3-103           253,440\n",
       "│    │    └─_DenseLayer: 3-104           257,600\n",
       "│    │    └─_DenseLayer: 3-105           261,760\n",
       "│    │    └─_DenseLayer: 3-106           265,920\n",
       "│    │    └─_DenseLayer: 3-107           270,080\n",
       "│    │    └─_DenseLayer: 3-108           274,240\n",
       "│    │    └─_DenseLayer: 3-109           278,400\n",
       "│    │    └─_DenseLayer: 3-110           282,560\n",
       "│    └─BatchNorm2d: 2-12                 3,840\n",
       "├─Linear: 1-2                            7,684\n",
       "=================================================================\n",
       "Total params: 18,100,612\n",
       "Trainable params: 18,100,612\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import models\n",
    "from torch import nn\n",
    "from torchinfo import summary\n",
    "\n",
    "MODEL_NAME = \"ADVANCED_FINE_TUNED\"\n",
    "N_CLASSES = 4\n",
    "\n",
    "def get_model(n_classes):\n",
    "    model = models.densenet201(pretrained=True)\n",
    "    in_features = model.classifier.in_features\n",
    "    model.classifier = nn.Linear(\n",
    "        in_features=in_features,\n",
    "        out_features=n_classes,\n",
    "        bias=True\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "model = get_model(n_classes=N_CLASSES)\n",
    "\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Optimizer, LR Scheduler, Loss function and Metric Scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import torchmetrics\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    params=model.parameters(),\n",
    "    lr=1e-3,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=5e-4,\n",
    "    amsgrad=True\n",
    ")\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.ExponentialLR(\n",
    "    optimizer=optimizer,\n",
    "    gamma=0.9\n",
    ")\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "metric_scorer = torchmetrics.Accuracy(\n",
    "    threshold=0.5,\n",
    "    num_classes=N_CLASSES,\n",
    "    average=\"macro\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Epoch Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "def epoch_iter(dataloader, model, loss_fn, device, is_train = True, optimizer=None, lr_scheduler=None):\n",
    "    if is_train:\n",
    "        assert optimizer is not None, \"When training, please provide an optimizer.\"\n",
    "      \n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    if is_train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    probs = []\n",
    "    preds = []\n",
    "    expected_labels = []\n",
    "    imageIds = []\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.set_grad_enabled(is_train):\n",
    "        for _batch, (X, y) in enumerate(tqdm(dataloader)):\n",
    "            labels = y[\"labels\"]\n",
    "            ids = y[\"imageIds\"]\n",
    "\n",
    "            X, y = X.to(device), labels.to(device)\n",
    "\n",
    "            pred = model(X)\n",
    "            final_pred = nn.Sigmoid()(pred)\n",
    "            \n",
    "            loss = loss_fn(final_pred, y.float())\n",
    "\n",
    "            if is_train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "\n",
    "            probs.extend(pred.detach().cpu().numpy())\n",
    "            preds.extend(final_pred.detach().cpu().numpy())\n",
    "            expected_labels.extend(y.detach().cpu().numpy())\n",
    "            imageIds.extend([f\"road{imageId}\" for imageId in ids.detach().cpu().numpy()])\n",
    "\n",
    "        if is_train and lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "    return (expected_labels, preds, probs, imageIds), total_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ADVANCED_FINE_TUNED training...\n",
      "Epoch[1/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:13<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.452\t Training macro accuracy: 0.838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:03<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.421\t Validation macro accuracy: 0.844\n",
      "----------------------------------------------------------------\n",
      "Epoch[2/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:09<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.383\t Training macro accuracy: 0.857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:04<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.369\t Validation macro accuracy: 0.883\n",
      "----------------------------------------------------------------\n",
      "Epoch[3/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:09<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.351\t Training macro accuracy: 0.876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:04<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.331\t Validation macro accuracy: 0.888\n",
      "----------------------------------------------------------------\n",
      "Epoch[4/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:09<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.340\t Training macro accuracy: 0.879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:04<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.315\t Validation macro accuracy: 0.888\n",
      "----------------------------------------------------------------\n",
      "Epoch[5/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:10<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.308\t Training macro accuracy: 0.887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:04<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.320\t Validation macro accuracy: 0.880\n",
      "----------------------------------------------------------------\n",
      "Epoch[6/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:10<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.305\t Training macro accuracy: 0.884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:04<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.304\t Validation macro accuracy: 0.888\n",
      "----------------------------------------------------------------\n",
      "Epoch[7/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:10<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.286\t Training macro accuracy: 0.894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:04<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.307\t Validation macro accuracy: 0.885\n",
      "----------------------------------------------------------------\n",
      "Epoch[8/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:10<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.282\t Training macro accuracy: 0.893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:04<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.282\t Validation macro accuracy: 0.904\n",
      "----------------------------------------------------------------\n",
      "Epoch[9/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:10<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.274\t Training macro accuracy: 0.889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:04<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.244\t Validation macro accuracy: 0.919\n",
      "----------------------------------------------------------------\n",
      "Epoch[10/30]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 6/15 [00:06<00:09,  1.06s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23732/1221964200.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch[{epoch}/{NUM_EPOCHS}]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     (train_target, train_preds, train_probs, _), train_loss = epoch_iter(\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0mdataloader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23732/3354154053.py\u001b[0m in \u001b[0;36mepoch_iter\u001b[1;34m(dataloader, model, loss_fn, device, is_train, optimizer, lr_scheduler)\u001b[0m\n\u001b[0;32m     38\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m             \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[0mprobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 30\n",
    "\n",
    "model.to(device)\n",
    "model.features.requires_grad_(False) # Freeze feature layer\n",
    "\n",
    "train_history = {\n",
    "    \"loss\": [],\n",
    "    \"accuracy\": [],\n",
    "}\n",
    "\n",
    "val_history = {\n",
    "    \"loss\": [],\n",
    "    \"accuracy\": [],\n",
    "}\n",
    "\n",
    "best_val_loss = np.inf\n",
    "best_val_accuracy = 0\n",
    "best_epoch = -1\n",
    "\n",
    "print(f\"Starting {MODEL_NAME} training...\")\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    print(f\"Epoch[{epoch}/{NUM_EPOCHS}]\")\n",
    "    (train_target, train_preds, train_probs, _), train_loss = epoch_iter(\n",
    "        dataloader=train_dataloader,\n",
    "        model=model,\n",
    "        loss_fn=loss_fn,\n",
    "        device=device,\n",
    "        is_train=True,\n",
    "        optimizer=optimizer,\n",
    "        lr_scheduler=lr_scheduler\n",
    "    )\n",
    "\n",
    "    train_accuracy = metric_scorer(torch.tensor(np.array(train_probs)), torch.tensor(np.array(train_target))).item()\n",
    "    print(f\"Training loss: {train_loss:.3f}\\t Training macro accuracy: {train_accuracy:.3f}\")\n",
    "\n",
    "    (val_target, val_preds, val_probs, _), val_loss = epoch_iter(\n",
    "        dataloader=val_dataloader,\n",
    "        model=model,\n",
    "        loss_fn=loss_fn,\n",
    "        device=device,\n",
    "        is_train=False,\n",
    "    )\n",
    "\n",
    "    val_accuracy = metric_scorer(torch.tensor(np.array(val_probs)), torch.tensor(np.array(val_target))).item()\n",
    "    print(f\"Validation loss: {val_loss:.3f}\\t Validation macro accuracy: {val_accuracy:.3f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_epoch = epoch\n",
    "        save_dict = {\"model\": model.state_dict(), \"optimizer\": optimizer.state_dict(), \"lr_scheduler\": lr_scheduler.state_dict(), \"epoch\": epoch}\n",
    "        torch.save(save_dict, f\"{OUT_DIR}/{MODEL_NAME}_best_model.pth\")\n",
    "\n",
    "    # Save latest model\n",
    "    save_dict = {\"model\": model.state_dict(), \"optimizer\": optimizer.state_dict(), \"lr_scheduler\": lr_scheduler.state_dict(), \"epoch\": epoch}\n",
    "    torch.save(save_dict, f\"{OUT_DIR}/{MODEL_NAME}_latest_model.pth\")\n",
    "\n",
    "    # Save loss and accuracy in history\n",
    "    train_history[\"loss\"].append(train_loss)\n",
    "    train_history[\"accuracy\"].append(train_accuracy)\n",
    "\n",
    "    val_history[\"loss\"].append(val_loss)\n",
    "    val_history[\"accuracy\"].append(val_accuracy)\n",
    "\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "\n",
    "print(\n",
    "    f\"\\nFinished training...\"\n",
    "    f\"\\nBest epoch: {best_epoch}\\t Validation loss on best epoch: {best_val_loss}\\t Accuracy on best epoch: {best_val_accuracy}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "706ee1819b983a943bbea807e6681581497887a5e921ebf85b777ef931d5d8ae"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
