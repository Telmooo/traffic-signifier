{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-Stage Object detection w/ Freeze Learning & Fine-tuning\n",
    "For this approach, it will be used a pretrained DensetNet201 as a backbone for a F-RCNN model for object detection, to which it will be applied freeze learning and fine-tuning to our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch \n",
    "import pandas as pd\n",
    "\n",
    "DATA_DIR = \"../data/\"\n",
    "IMG_DIR = DATA_DIR + \"/images/\"\n",
    "ANNOTATION_DIR = DATA_DIR + \"/annotations/\"\n",
    "SPLITS_DIR = DATA_DIR + \"/dl-split/\"\n",
    "OUT_DIR = \"./out/yolo/\"\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ultralytics/yolov5 models/yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirements\n",
    "!pip install -r models/yolov5/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "\n",
    "np.random.seed(SEED)\n",
    "\n",
    "train_names = []\n",
    "with open('../data/dl-split/train.txt', 'r') as train_f:\n",
    "    train_names = train_f.readlines()\n",
    "    train_names = [x.strip() for x in train_names]\n",
    "\n",
    "train_indices = list(range(len(train_names)))\n",
    "np.random.shuffle(train_indices)\n",
    "train_val_split = int(np.floor(0.2 * len(train_indices)))\n",
    "\n",
    "train_idx, val_idx = train_indices[train_val_split:], train_indices[:train_val_split]\n",
    "\n",
    "val_data = [train_names[idx] for idx in val_idx]\n",
    "train_data = [train_names[idx] for idx in train_idx]\n",
    "\n",
    "test_data = []\n",
    "with open('../data/dl-split/test.txt', 'r') as test_f:\n",
    "    test_data = test_f.readlines()\n",
    "    test_data = [x.strip() for x in test_data]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import parse_annotation\n",
    "import shutil\n",
    "\n",
    "TRAIN_DIR='../data/images/train'\n",
    "TEST_DIR='../data/images/test'\n",
    "VAL_DIR='../data/images/val'\n",
    "\n",
    "TRAIN_LABELS='../data/labels/train'\n",
    "TEST_LABELS='../data/labels/test'\n",
    "VAL_LABELS='../data/labels/val'\n",
    "\n",
    "os.makedirs(TRAIN_DIR, exist_ok=True)\n",
    "os.makedirs(TEST_DIR, exist_ok=True)\n",
    "os.makedirs(VAL_DIR, exist_ok=True)\n",
    "\n",
    "os.makedirs(TRAIN_LABELS, exist_ok=True)\n",
    "os.makedirs(TEST_LABELS, exist_ok=True)\n",
    "os.makedirs(VAL_LABELS, exist_ok=True)\n",
    "\n",
    "label_encode_map = {\n",
    "    \"trafficlight\": 0,\n",
    "    \"speedlimit\": 1,\n",
    "    \"crosswalk\": 2,\n",
    "    \"stop\": 3,\n",
    "}\n",
    "\n",
    "def transform_labels(out_path: str, annot_dict):\n",
    "    annot_file = open(out_path, 'w')\n",
    "    \n",
    "    img_width = annot_dict['width']\n",
    "    img_height = annot_dict['height']\n",
    "    for label, box in zip(annot_dict['labels'], annot_dict['boxes']):\n",
    "        width = (box[2]-box[0])/img_width\n",
    "        height = (box[3]-box[1])/img_height\n",
    "        \n",
    "        x_center = (box[0]+box[2])/2/img_width\n",
    "        y_center = (box[3]+box[1])/2/img_height\n",
    "        \n",
    "        annot_file.write(f'{label} {x_center} {y_center} {width} {height}\\n')\n",
    "    annot_file.close()\n",
    "\n",
    "# training data\n",
    "for name in train_data:\n",
    "    annot_dict = parse_annotation(f'../data/annotations/{name}.xml', label_encode_map)\n",
    "    transform_labels(f'{TRAIN_LABELS}/{name}.txt', annot_dict)\n",
    "    shutil.copy(src=f'../data/images/{name}.png', dst=TRAIN_DIR)\n",
    "  \n",
    "# validation data      \n",
    "for name in val_data:\n",
    "    annot_dict = parse_annotation(f'../data/annotations/{name}.xml', label_encode_map)\n",
    "    transform_labels(f'{VAL_LABELS}/{name}.txt', annot_dict)\n",
    "    shutil.copy(src=f'../data/images/{name}.png', dst=VAL_DIR)\n",
    " \n",
    "# test data       \n",
    "for name in test_data:\n",
    "    annot_dict = parse_annotation(f'../data/annotations/{name}.xml', label_encode_map)\n",
    "    transform_labels(f'{TEST_LABELS}/{name}.txt', annot_dict)\n",
    "    shutil.copy(src=f'../data/images/{name}.png', dst=TEST_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: train.py [-h] [--weights WEIGHTS] [--cfg CFG] [--data DATA] [--hyp HYP]\n",
      "                [--epochs EPOCHS] [--batch-size BATCH_SIZE] [--imgsz IMGSZ]\n",
      "                [--rect] [--resume [RESUME]] [--nosave] [--noval]\n",
      "                [--noautoanchor] [--noplots] [--evolve [EVOLVE]]\n",
      "                [--bucket BUCKET] [--cache [CACHE]] [--image-weights]\n",
      "                [--device DEVICE] [--multi-scale] [--single-cls]\n",
      "                [--optimizer {SGD,Adam,AdamW}] [--sync-bn] [--workers WORKERS]\n",
      "                [--project PROJECT] [--name NAME] [--exist-ok] [--quad]\n",
      "                [--cos-lr] [--label-smoothing LABEL_SMOOTHING]\n",
      "                [--patience PATIENCE] [--freeze FREEZE [FREEZE ...]]\n",
      "                [--save-period SAVE_PERIOD] [--local_rank LOCAL_RANK]\n",
      "                [--entity ENTITY] [--upload_dataset [UPLOAD_DATASET]]\n",
      "                [--bbox_interval BBOX_INTERVAL]\n",
      "                [--artifact_alias ARTIFACT_ALIAS]\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --weights WEIGHTS     initial weights path\n",
      "  --cfg CFG             model.yaml path\n",
      "  --data DATA           dataset.yaml path\n",
      "  --hyp HYP             hyperparameters path\n",
      "  --epochs EPOCHS\n",
      "  --batch-size BATCH_SIZE\n",
      "                        total batch size for all GPUs, -1 for autobatch\n",
      "  --imgsz IMGSZ, --img IMGSZ, --img-size IMGSZ\n",
      "                        train, val image size (pixels)\n",
      "  --rect                rectangular training\n",
      "  --resume [RESUME]     resume most recent training\n",
      "  --nosave              only save final checkpoint\n",
      "  --noval               only validate final epoch\n",
      "  --noautoanchor        disable AutoAnchor\n",
      "  --noplots             save no plot files\n",
      "  --evolve [EVOLVE]     evolve hyperparameters for x generations\n",
      "  --bucket BUCKET       gsutil bucket\n",
      "  --cache [CACHE]       --cache images in \"ram\" (default) or \"disk\"\n",
      "  --image-weights       use weighted image selection for training\n",
      "  --device DEVICE       cuda device, i.e. 0 or 0,1,2,3 or cpu\n",
      "  --multi-scale         vary img-size +/- 50%\n",
      "  --single-cls          train multi-class data as single-class\n",
      "  --optimizer {SGD,Adam,AdamW}\n",
      "                        optimizer\n",
      "  --sync-bn             use SyncBatchNorm, only available in DDP mode\n",
      "  --workers WORKERS     max dataloader workers (per RANK in DDP mode)\n",
      "  --project PROJECT     save to project/name\n",
      "  --name NAME           save to project/name\n",
      "  --exist-ok            existing project/name ok, do not increment\n",
      "  --quad                quad dataloader\n",
      "  --cos-lr              cosine LR scheduler\n",
      "  --label-smoothing LABEL_SMOOTHING\n",
      "                        Label smoothing epsilon\n",
      "  --patience PATIENCE   EarlyStopping patience (epochs without improvement)\n",
      "  --freeze FREEZE [FREEZE ...]\n",
      "                        Freeze layers: backbone=10, first3=0 1 2\n",
      "  --save-period SAVE_PERIOD\n",
      "                        Save checkpoint every x epochs (disabled if < 1)\n",
      "  --local_rank LOCAL_RANK\n",
      "                        DDP parameter, do not modify\n",
      "  --entity ENTITY       W&B: Entity\n",
      "  --upload_dataset [UPLOAD_DATASET]\n",
      "                        W&B: Upload data, \"val\" option\n",
      "  --bbox_interval BBOX_INTERVAL\n",
      "                        W&B: Set bounding-box image logging interval\n",
      "  --artifact_alias ARTIFACT_ALIAS\n",
      "                        W&B: Version of dataset artifact to use\n"
     ]
    }
   ],
   "source": [
    "!python models/yolov5/train.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python models/yolov5/train.py --batch 4 --epochs 15 --data models/yolo_cfg.yaml --workers 2 --project out/yolo/ --optimizer SGD --hyp models/hyp.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: val.py [-h] [--data DATA] [--weights WEIGHTS [WEIGHTS ...]]\n",
      "              [--batch-size BATCH_SIZE] [--imgsz IMGSZ]\n",
      "              [--conf-thres CONF_THRES] [--iou-thres IOU_THRES] [--task TASK]\n",
      "              [--device DEVICE] [--workers WORKERS] [--single-cls] [--augment]\n",
      "              [--verbose] [--save-txt] [--save-hybrid] [--save-conf]\n",
      "              [--save-json] [--project PROJECT] [--name NAME] [--exist-ok]\n",
      "              [--half] [--dnn]\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --data DATA           dataset.yaml path\n",
      "  --weights WEIGHTS [WEIGHTS ...]\n",
      "                        model.pt path(s)\n",
      "  --batch-size BATCH_SIZE\n",
      "                        batch size\n",
      "  --imgsz IMGSZ, --img IMGSZ, --img-size IMGSZ\n",
      "                        inference size (pixels)\n",
      "  --conf-thres CONF_THRES\n",
      "                        confidence threshold\n",
      "  --iou-thres IOU_THRES\n",
      "                        NMS IoU threshold\n",
      "  --task TASK           train, val, test, speed or study\n",
      "  --device DEVICE       cuda device, i.e. 0 or 0,1,2,3 or cpu\n",
      "  --workers WORKERS     max dataloader workers (per RANK in DDP mode)\n",
      "  --single-cls          treat as single-class dataset\n",
      "  --augment             augmented inference\n",
      "  --verbose             report mAP by class\n",
      "  --save-txt            save results to *.txt\n",
      "  --save-hybrid         save label+prediction hybrid results to *.txt\n",
      "  --save-conf           save confidences in --save-txt labels\n",
      "  --save-json           save a COCO-JSON results file\n",
      "  --project PROJECT     save to project/name\n",
      "  --name NAME           save to project/name\n",
      "  --exist-ok            existing project/name ok, do not increment\n",
      "  --half                use FP16 half-precision inference\n",
      "  --dnn                 use OpenCV DNN for ONNX inference\n"
     ]
    }
   ],
   "source": [
    "!python models/yolov5/val.py -h "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python models/yolov5/val.py --workers 2 --verbose --task test --weights out/yolo/exp/weights/best.pt --data models/yolo_cfg.yaml --batch-size 4 --conf-thres=0.25 --iou-thres 0.45  --save-json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: detect.py [-h] [--weights WEIGHTS [WEIGHTS ...]] [--source SOURCE]\n",
      "                 [--data DATA] [--imgsz IMGSZ [IMGSZ ...]]\n",
      "                 [--conf-thres CONF_THRES] [--iou-thres IOU_THRES]\n",
      "                 [--max-det MAX_DET] [--device DEVICE] [--view-img]\n",
      "                 [--save-txt] [--save-conf] [--save-crop] [--nosave]\n",
      "                 [--classes CLASSES [CLASSES ...]] [--agnostic-nms]\n",
      "                 [--augment] [--visualize] [--update] [--project PROJECT]\n",
      "                 [--name NAME] [--exist-ok] [--line-thickness LINE_THICKNESS]\n",
      "                 [--hide-labels] [--hide-conf] [--half] [--dnn]\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --weights WEIGHTS [WEIGHTS ...]\n",
      "                        model path(s)\n",
      "  --source SOURCE       file/dir/URL/glob, 0 for webcam\n",
      "  --data DATA           (optional) dataset.yaml path\n",
      "  --imgsz IMGSZ [IMGSZ ...], --img IMGSZ [IMGSZ ...], --img-size IMGSZ [IMGSZ ...]\n",
      "                        inference size h,w\n",
      "  --conf-thres CONF_THRES\n",
      "                        confidence threshold\n",
      "  --iou-thres IOU_THRES\n",
      "                        NMS IoU threshold\n",
      "  --max-det MAX_DET     maximum detections per image\n",
      "  --device DEVICE       cuda device, i.e. 0 or 0,1,2,3 or cpu\n",
      "  --view-img            show results\n",
      "  --save-txt            save results to *.txt\n",
      "  --save-conf           save confidences in --save-txt labels\n",
      "  --save-crop           save cropped prediction boxes\n",
      "  --nosave              do not save images/videos\n",
      "  --classes CLASSES [CLASSES ...]\n",
      "                        filter by class: --classes 0, or --classes 0 2 3\n",
      "  --agnostic-nms        class-agnostic NMS\n",
      "  --augment             augmented inference\n",
      "  --visualize           visualize features\n",
      "  --update              update all models\n",
      "  --project PROJECT     save results to project/name\n",
      "  --name NAME           save results to project/name\n",
      "  --exist-ok            existing project/name ok, do not increment\n",
      "  --line-thickness LINE_THICKNESS\n",
      "                        bounding box thickness (pixels)\n",
      "  --hide-labels         hide labels\n",
      "  --hide-conf           hide confidences\n",
      "  --half                use FP16 half-precision inference\n",
      "  --dnn                 use OpenCV DNN for ONNX inference\n"
     ]
    }
   ],
   "source": [
    "!python models/yolov5/detect.py -h "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python models/yolov5/detect.py --weights out/yolo/exp/weights/best.pt --source ../data/images/test/*.png --project out/yolo/runs --save-txt --conf-thres 0.25 --save-conf"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "706ee1819b983a943bbea807e6681581497887a5e921ebf85b777ef931d5d8ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
