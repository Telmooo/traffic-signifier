{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Version w/ Custom Architecture\n",
    "For this approach, it will be used a custom architecture, inspired by Inception and Densenet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_DIR = \"../data/\"\n",
    "IMG_DIR = DATA_DIR + \"/images/\"\n",
    "ANNOTATION_DIR = DATA_DIR + \"/annotations/\"\n",
    "SPLITS_DIR = DATA_DIR + \"/dl-split/\"\n",
    "OUT_DIR = \"./out/basic_custom/\"\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_training_history(train_history, val_history, model_name, phase_name, out_dir):\n",
    "    fig, (loss_ax, acc_ax) = plt.subplots(figsize=(12, 8), nrows=2)\n",
    "    fig.suptitle(f\"{model_name} - {phase_name} History\")\n",
    "    loss_ax.set_title(\"Cross Entropy Loss\")\n",
    "    loss_ax.plot(train_history[\"loss\"], label=\"train\")\n",
    "    loss_ax.plot(val_history[\"loss\"], label=\"val\")\n",
    "    loss_ax.legend(loc=\"best\")\n",
    "\n",
    "    acc_ax.set_title(\"Classification accuracy\")\n",
    "    acc_ax.plot(train_history[\"accuracy\"], label=\"train\")\n",
    "    acc_ax.plot(val_history[\"accuracy\"], label=\"val\")\n",
    "    loss_ax.legend(loc=\"best\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fig.savefig(f\"{out_dir}/{model_name}_{phase_name}_history.png\", dpi=150, bbox_inches='tight')\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching pre-defined splits\n",
    "train_split = []\n",
    "test_split = []\n",
    "\n",
    "with open(SPLITS_DIR + \"/train.txt\") as train_split_f:\n",
    "    train_split = [line.strip(\"\\n\") for line in train_split_f.readlines()]\n",
    "\n",
    "with open(SPLITS_DIR + \"/test.txt\") as test_split_f:\n",
    "    test_split = [line.strip(\"\\n\") for line in test_split_f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label mapping\n",
    "label_encode_map = {\n",
    "    \"background\": -100,\n",
    "    \"trafficlight\": 0,\n",
    "    \"speedlimit\": 1,\n",
    "    \"crosswalk\": 2,\n",
    "    \"stop\": 3,\n",
    "}\n",
    "\n",
    "label_decode_map = {\n",
    "    -100: \"background\",\n",
    "    0: \"trafficlight\",\n",
    "    1: \"speedlimit\",\n",
    "    2: \"crosswalk\",\n",
    "    3: \"stop\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cull training dataset to balance classes better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import parse_annotation\n",
    "\n",
    "def get_annotations(split):\n",
    "    annotations = {}\n",
    "    for id in split:\n",
    "        annotation = parse_annotation(f\"{ANNOTATION_DIR}/{id}.xml\", label_encode_map, return_biggest = True)\n",
    "        annotation[\"labels\"] = annotation[\"labels\"][0]\n",
    "        annotations[id] = annotation\n",
    "\n",
    "    return annotations\n",
    "\n",
    "annotations = get_annotations(train_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.DataFrame.from_dict(annotations, orient=\"index\")\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Label Ratios\")\n",
    "label_ratios = (train_df[\"labels\"].value_counts() / len(train_df[\"labels\"])).sort_index()\n",
    "print(label_ratios)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.countplot(data=train_df, x=\"labels\", ax=ax, tick_label=[\"trafficlight\", \"speedlimit\", \"crosswalk\", \"stop\"])\n",
    "ax.set_title(\"Label Distribution\")\n",
    "\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "non_max_labels = label_ratios[label_ratios != label_ratios.max()]\n",
    "ratio_to_return = np.sum(non_max_labels) + np.max(non_max_labels) * 2\n",
    "\n",
    "print(f\"Ratio of training dataset to return: {ratio_to_return}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ratios[1] /= 2\n",
    "label_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = (1 - label_ratios) / np.sum(1 - label_ratios)\n",
    "print(\"Weights\")\n",
    "print(weights)\n",
    "train_df[\"weights\"] = 0\n",
    "for i, weight in enumerate(weights):\n",
    "    train_df.loc[train_df[\"labels\"] == i, \"weights\"] = weight / len(train_df[train_df[\"labels\"] == i])\n",
    "\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "culled_df = train_df.sample(frac=ratio_to_return, weights=\"weights\", random_state=SEED)\n",
    "print(\"Label Ratios\")\n",
    "label_ratios = (culled_df[\"labels\"].value_counts() / len(culled_df[\"labels\"])).sort_index()\n",
    "print(label_ratios)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.countplot(data=culled_df, x=\"labels\", ax=ax, tick_label=[\"trafficlight\", \"speedlimit\", \"crosswalk\", \"stop\"])\n",
    "ax.set_title(\"Label Distribution on culled training dataset\")\n",
    "\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "culled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maintain the ratios of labels across splits\n",
    "train_sample = culled_df.groupby(\"labels\").sample(frac=0.8, random_state=SEED)\n",
    "val_sample = culled_df.loc[culled_df.index.difference(train_sample.index)]\n",
    "\n",
    "fig, (train_ax, val_ax) = plt.subplots(figsize=(16, 4), ncols=2)\n",
    "for split, df, ax in zip([\"Train\", \"Validation\"], [train_sample, val_sample], [train_ax, val_ax]):\n",
    "    sns.countplot(x=df[\"labels\"], ax=ax)\n",
    "    ax.set_title(f\"Distribution of labels on {split} sample\")\n",
    "    ax.set_xlabel(\"Label\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    \n",
    "    print(f\"{split} Label Ratios\")\n",
    "    print(pd.Series(df[\"labels\"]).value_counts() / len(df[\"labels\"]))\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.road_sign_dataset import RoadSignDataset\n",
    "\n",
    "# Training dataset\n",
    "training_data = RoadSignDataset(\n",
    "    img_names=train_sample.index.tolist(),\n",
    "    img_dir=IMG_DIR,\n",
    "    annotation_dir=ANNOTATION_DIR,\n",
    "    classes=label_encode_map,\n",
    "    is_train=True,\n",
    "    multilabel=False\n",
    ")\n",
    "\n",
    "# Validation dataset\n",
    "validation_data = RoadSignDataset(\n",
    "    img_names=val_sample.index.tolist(),\n",
    "    img_dir=IMG_DIR,\n",
    "    annotation_dir=ANNOTATION_DIR,\n",
    "    classes=label_encode_map,\n",
    "    is_train=True,\n",
    "    multilabel=False\n",
    ")\n",
    "\n",
    "# Test dataset\n",
    "testing_data = RoadSignDataset(\n",
    "    img_names=test_split,\n",
    "    img_dir=IMG_DIR,\n",
    "    annotation_dir=ANNOTATION_DIR,\n",
    "    classes=label_encode_map,\n",
    "    is_train=False,\n",
    "    multilabel=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 8 # Tested on 1050TI with 4GB (can load at least 64 as well, but doesn't make sense to use 64 with low amount of data)\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=training_data,\n",
    "    #sampler=train_sampler,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=training_data.collate_fn\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    dataset=validation_data,\n",
    "    #sampler=val_sampler,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=training_data.collate_fn\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=testing_data,\n",
    "    batch_size=1,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=testing_data.collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.TrafficSignifier import TrafficSignifier\n",
    "from torch import nn\n",
    "from torchinfo import summary\n",
    "\n",
    "MODEL_NAME = \"BASIC_CUSTOM\"\n",
    "N_CLASSES = 4\n",
    "\n",
    "def get_model():\n",
    "    return TrafficSignifier(\n",
    "        num_classes=N_CLASSES,\n",
    "        num_blocks=6,\n",
    "        num_internal_layers=4,\n",
    "    )\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Optimizer, LR Scheduler, Loss function and Metric Scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import torchmetrics\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    params=model.parameters(),\n",
    "    lr=1e-2,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=5e-4,\n",
    "    amsgrad=True\n",
    ")\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.ExponentialLR(\n",
    "    optimizer=optimizer,\n",
    "    gamma=0.99,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=label_encode_map[\"background\"])\n",
    "\n",
    "metric_scorer = torchmetrics.Accuracy(\n",
    "    threshold=0.5,\n",
    "    num_classes=N_CLASSES,\n",
    "    average=\"micro\",\n",
    "    ignore_index=label_encode_map[\"background\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Epoch Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "def epoch_iter(dataloader, model, loss_fn, device, is_train = True, optimizer=None, lr_scheduler=None):\n",
    "    if is_train:\n",
    "        assert optimizer is not None, \"When training, please provide an optimizer.\"\n",
    "      \n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    if is_train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    probs = []\n",
    "    preds = []\n",
    "    expected_labels = []\n",
    "    imageIds = []\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    with torch.set_grad_enabled(is_train):\n",
    "        for _batch, (X, y) in enumerate(tqdm(dataloader)):\n",
    "            labels = y[\"labels\"]\n",
    "            ids = y[\"imageIds\"]\n",
    "\n",
    "            X, y = X.to(device), labels.to(device)\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                pred = model(X)\n",
    "                loss = loss_fn(pred, y)\n",
    "\n",
    "            if is_train:\n",
    "                optimizer.zero_grad()\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "\n",
    "\n",
    "            prob = F.softmax(pred, dim=1)\n",
    "            final_pred = torch.argmax(prob, dim=1)\n",
    "\n",
    "            probs.extend(prob.detach().cpu().numpy())\n",
    "            preds.extend(final_pred.detach().cpu().numpy())\n",
    "            expected_labels.extend(y.detach().cpu().numpy())\n",
    "            imageIds.extend([f\"road{imageId}\" for imageId in ids.detach().cpu().numpy()])\n",
    "\n",
    "        if is_train and lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "    \n",
    "\n",
    "    return (expected_labels, preds, probs, imageIds), total_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "model.to(device)    \n",
    "# model.features.requires_grad_(False) # Freeze feature layer\n",
    "\n",
    "train_history = {\n",
    "    \"loss\": [],\n",
    "    \"accuracy\": [],\n",
    "}\n",
    "\n",
    "val_history = {\n",
    "    \"loss\": [],\n",
    "    \"accuracy\": [],\n",
    "}\n",
    "\n",
    "best_val_loss = np.inf\n",
    "best_val_accuracy = 0\n",
    "best_epoch = -1\n",
    "\n",
    "print(f\"Starting {MODEL_NAME} training...\")\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    print(f\"Epoch[{epoch}/{NUM_EPOCHS}]\")\n",
    "    (train_target, train_preds, train_probs, _), train_loss = epoch_iter(\n",
    "        dataloader=train_dataloader,\n",
    "        model=model,\n",
    "        loss_fn=loss_fn,\n",
    "        device=device,\n",
    "        is_train=True,\n",
    "        optimizer=optimizer,\n",
    "        lr_scheduler=lr_scheduler\n",
    "    )\n",
    "\n",
    "    train_accuracy = metric_scorer(torch.tensor(np.array(train_probs)), torch.tensor(np.array(train_target))).item()\n",
    "    print(f\"Training loss: {train_loss:.3f}\\t Training micro accuracy: {train_accuracy:.3f}\")\n",
    "\n",
    "    (val_target, val_preds, val_probs, _), val_loss = epoch_iter(\n",
    "        dataloader=val_dataloader,\n",
    "        model=model,\n",
    "        loss_fn=loss_fn,\n",
    "        device=device,\n",
    "        is_train=False,\n",
    "    )\n",
    "\n",
    "    val_accuracy = metric_scorer(torch.tensor(np.array(val_probs)), torch.tensor(np.array(val_target))).item()\n",
    "    print(f\"Validation loss: {val_loss:.3f}\\t Validation micro accuracy: {val_accuracy:.3f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_epoch = epoch\n",
    "        save_dict = {\"model\": model.state_dict(), \"optimizer\": optimizer.state_dict(), \"lr_scheduler\": lr_scheduler.state_dict(), \"epoch\": epoch}\n",
    "        torch.save(save_dict, f\"{OUT_DIR}/{MODEL_NAME}_best_model.pth\")\n",
    "\n",
    "    # Save latest model\n",
    "    save_dict = {\"model\": model.state_dict(), \"optimizer\": optimizer.state_dict(), \"lr_scheduler\": lr_scheduler.state_dict(), \"epoch\": epoch}\n",
    "    torch.save(save_dict, f\"{OUT_DIR}/{MODEL_NAME}_latest_model.pth\")\n",
    "\n",
    "    # Save loss and accuracy in history\n",
    "    train_history[\"loss\"].append(train_loss)\n",
    "    train_history[\"accuracy\"].append(train_accuracy)\n",
    "\n",
    "    val_history[\"loss\"].append(val_loss)\n",
    "    val_history[\"accuracy\"].append(val_accuracy)\n",
    "\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "\n",
    "print(\n",
    "    f\"\\nFinished training...\"\n",
    "    f\"\\nBest epoch: {best_epoch}\\t Validation loss on best epoch: {best_val_loss}\\t Accuracy on best epoch: {best_val_accuracy}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_training_history(train_history=train_history, val_history=val_history, model_name=MODEL_NAME, phase_name=\"Training\", out_dir=OUT_DIR)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clear GPU memory for guarantees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "model = None\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.mem_get_info(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load best model from first training session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8 # Have to reduce batch size otherwise GPU memory dies (tested on 1050TI with 4GB)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=training_data,\n",
    "    sampler=train_sampler,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    shuffle=False, # Must be False because we're using a random sampler already\n",
    "    drop_last=True,\n",
    "    collate_fn=training_data.collate_fn\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    dataset=training_data,\n",
    "    sampler=val_sampler,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    shuffle=False, # Must be False because we're using a random sampler already\n",
    "    drop_last=True,\n",
    "    collate_fn=training_data.collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_checkpoint = torch.load(f\"{OUT_DIR}/{MODEL_NAME}_best_model.pth\")\n",
    "model = get_model()\n",
    "model.load_state_dict(best_checkpoint[\"model\"])\n",
    "best_checkpoint = None\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "print(\"Loaded best model...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_optimizer = optim.Adam(\n",
    "    params=model.parameters(),\n",
    "    lr=1e-3,\n",
    "    betas=(0.9, 0.999),\n",
    "    weight_decay=5e-4,\n",
    "    amsgrad=True\n",
    ")\n",
    "\n",
    "ft_lr_scheduler = optim.lr_scheduler.ExponentialLR(\n",
    "    optimizer=ft_optimizer,\n",
    "    gamma=0.99\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FT_NUM_EPOCHS = 30\n",
    "#model.features.requires_grad_(True) # Unfreeze feature layer for fine-tuning\n",
    "\n",
    "ft_train_history = {\n",
    "    \"loss\": [],\n",
    "    \"accuracy\": [],\n",
    "}\n",
    "\n",
    "ft_val_history = {\n",
    "    \"loss\": [],\n",
    "    \"accuracy\": [],\n",
    "}\n",
    "\n",
    "ft_best_val_loss = best_val_loss\n",
    "ft_best_val_accuracy = best_val_accuracy\n",
    "ft_best_epoch = -1\n",
    "\n",
    "print(f\"Starting {MODEL_NAME} fine-tuning...\")\n",
    "\n",
    "for epoch in range(1, FT_NUM_EPOCHS + 1):\n",
    "    print(f\"Epoch[{epoch}/{FT_NUM_EPOCHS}]\")\n",
    "    (train_target, train_preds, train_probs, _), train_loss = epoch_iter(\n",
    "        dataloader=train_dataloader,\n",
    "        model=model,\n",
    "        loss_fn=loss_fn,\n",
    "        device=device,\n",
    "        is_train=True,\n",
    "        optimizer=ft_optimizer,\n",
    "        lr_scheduler=ft_lr_scheduler\n",
    "    )\n",
    "\n",
    "    train_accuracy = metric_scorer(torch.tensor(np.array(train_probs)), torch.tensor(np.array(train_target))).item()\n",
    "    print(f\"Training loss: {train_loss:.3f}\\t Training micro accuracy: {train_accuracy:.3f}\")\n",
    "\n",
    "    (val_target, val_preds, val_probs, _), val_loss = epoch_iter(\n",
    "        dataloader=val_dataloader,\n",
    "        model=model,\n",
    "        loss_fn=loss_fn,\n",
    "        device=device,\n",
    "        is_train=False,\n",
    "    )\n",
    "\n",
    "    val_accuracy = metric_scorer(torch.tensor(np.array(val_probs)), torch.tensor(np.array(val_target))).item()\n",
    "    print(f\"Validation loss: {val_loss:.3f}\\t Validation micro accuracy: {val_accuracy:.3f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_loss < ft_best_val_loss:\n",
    "        ft_best_val_loss = val_loss\n",
    "        ft_best_val_accuracy = val_accuracy\n",
    "        ft_best_epoch = epoch\n",
    "        save_dict = {\"model\": model.state_dict(), \"optimizer\": ft_optimizer.state_dict(), \"lr_scheduler\": ft_lr_scheduler.state_dict(), \"epoch\": epoch}\n",
    "        torch.save(save_dict, f\"{OUT_DIR}/{MODEL_NAME}_ft_best_model.pth\")\n",
    "\n",
    "    # Save latest model\n",
    "    save_dict = {\"model\": model.state_dict(), \"optimizer\": ft_optimizer.state_dict(), \"lr_scheduler\": ft_lr_scheduler.state_dict(), \"epoch\": epoch}\n",
    "    torch.save(save_dict, f\"{OUT_DIR}/{MODEL_NAME}_ft_latest_model.pth\")\n",
    "\n",
    "    # Save loss and accuracy in history\n",
    "    ft_train_history[\"loss\"].append(train_loss)\n",
    "    ft_train_history[\"accuracy\"].append(train_accuracy)\n",
    "\n",
    "    ft_val_history[\"loss\"].append(val_loss)\n",
    "    ft_val_history[\"accuracy\"].append(val_accuracy)\n",
    "\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "\n",
    "print(\n",
    "    f\"\\nFinished fine-tuning...\"\n",
    "    f\"\\nBest epoch: {ft_best_epoch}\\t Validation loss on best epoch: {ft_best_val_loss}\\t Accuracy on best epoch: {ft_best_val_accuracy}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_training_history(train_history=ft_train_history, val_history=ft_val_history, model_name=MODEL_NAME, phase_name=\"FINE-TUNE\", out_dir=OUT_DIR)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean GPU memory again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.mem_get_info(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_checkpoint = torch.load(f\"{OUT_DIR}/{MODEL_NAME}_best_model.pth\")\n",
    "model = get_model()\n",
    "model.load_state_dict(best_checkpoint[\"model\"])\n",
    "best_checkpoint = None\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "print(\"Loaded best fine-tuned model...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(test_target, test_preds, test_probs, test_ids), test_loss = epoch_iter(\n",
    "    dataloader=test_dataloader,\n",
    "    model=model,\n",
    "    loss_fn=loss_fn,\n",
    "    device=device,\n",
    "    is_train=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = torchmetrics.MetricCollection(\n",
    "    metrics={\n",
    "        \"micro_accuracy\": metric_scorer,\n",
    "        \"macro_accuracy\": torchmetrics.Accuracy(\n",
    "            threshold=0.5,\n",
    "            num_classes=N_CLASSES,\n",
    "            average=\"macro\",\n",
    "        ),\n",
    "        \"weighted_accuracy\": torchmetrics.Accuracy(\n",
    "            threshold=0.5,\n",
    "            num_classes=N_CLASSES,\n",
    "            average=\"weighted\",\n",
    "        ),\n",
    "        \"micro_f1_score\": torchmetrics.F1Score(\n",
    "            threshold=0.5,\n",
    "            num_classes=N_CLASSES,\n",
    "            average=\"micro\",\n",
    "        ),\n",
    "        \"macro_f1_score\": torchmetrics.F1Score(\n",
    "            threshold=0.5,\n",
    "            num_classes=N_CLASSES,\n",
    "            average=\"macro\",\n",
    "        ),\n",
    "        \"weighted_f1_score\": torchmetrics.F1Score(\n",
    "            threshold=0.5,\n",
    "            num_classes=N_CLASSES,\n",
    "            average=\"weighted\",\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "test_probs_tensor = torch.tensor(np.array(test_probs))\n",
    "test_target_tensor = torch.tensor(np.array(test_target))\n",
    "\n",
    "test_metrics_scores = test_metrics(test_probs_tensor, test_target_tensor)\n",
    "\n",
    "print(test_metrics_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true=test_target, y_pred=test_preds, target_names=list(label_encode_map.keys())[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def showErrors(imageIds, y_true, y_pred, limit=None, img_dir = \"./\"):\n",
    "    N = limit if limit is not None else len(imageIds)\n",
    "    N_COLS = 4\n",
    "    N_ROWS = N // 4 + N % 4\n",
    "    shown = 0\n",
    "    fig = plt.figure(figsize=(8*N_COLS, 12*N_ROWS))\n",
    "    for (imageId, correct, predicted) in zip(imageIds, y_true, y_pred):\n",
    "        if correct == predicted:\n",
    "            continue\n",
    "        image = np.array(Image.open(f\"{img_dir}/{imageId}.png\").convert(\"RGB\"))\n",
    "        target_label = f\"TRUE: {label_decode_map[correct]}\"\n",
    "        predicted_label = f\"PRED: {label_decode_map[predicted]}\"\n",
    "\n",
    "        ax = fig.add_subplot(N_ROWS, N_COLS, shown + 1)\n",
    "        ax.axis(\"off\")\n",
    "        ax.text(0, -8, imageId, fontsize=14, color=\"black\")\n",
    "        ax.text((len(imageId) + 1) * 8, -8, target_label, fontsize=14, color='green') # correct\n",
    "        ax.text((len(imageId) + len(target_label) + 1) * 8, -8, predicted_label, fontsize=14, color='red')  # predicted\n",
    "        ax.imshow(image)\n",
    "        \n",
    "        shown += 1\n",
    "\n",
    "        if shown >= N:\n",
    "            break\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showErrors(test_ids, test_target, test_preds, limit=None, img_dir=IMG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "706ee1819b983a943bbea807e6681581497887a5e921ebf85b777ef931d5d8ae"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
